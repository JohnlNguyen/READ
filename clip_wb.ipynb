{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff793a2-4c3f-473d-b4db-109dac7c8845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/data/home/ngjhn/read\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaccbef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac75a05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "\n",
    "sweeps = [\n",
    "    'y3ya5bpc', 'qb5zkdsx', 'l72dh1w7', 'kuvylfn5', 'e6rkfmy6', 'ucanqlze',\n",
    "    'ry0ss5vc', 'leziylwb', 'e63yxyoh', 'n13bozse', '1ybj6xzh', 'jk2b8bo6',\n",
    "    'pawx9wi3', 'kxl2l1qj', '78xmz3k9', 'eizv5hgc', 'a67196fk', '9pyz272w'\n",
    "]\n",
    "\n",
    "summary_list, config_list, type_list, val_loss_list = [], [], [], []\n",
    "for sweep in sweeps:\n",
    "    try:\n",
    "        s = api.sweep(f\"read/read/sweeps/{sweep}\")\n",
    "    except:\n",
    "        continue\n",
    "    for run in s.runs:\n",
    "        his = run.history()\n",
    "        if 'CIDEr' in his:\n",
    "            his = his[~his['CIDEr'].isna()]\n",
    "            cider = max(his['CIDEr'].max(), run.summary['CIDEr'])\n",
    "            val_loss = his['val_loss'].min()\n",
    "            lr = run.config['learning_rate']\n",
    "\n",
    "            summary_list.append(cider)\n",
    "            config_list.append(lr)\n",
    "            type_list.append(s.name)\n",
    "            val_loss_list.append(val_loss)\n",
    "\n",
    "runs_df = pd.DataFrame({\n",
    "    \"CIDEr\": summary_list,\n",
    "    \"lr\": config_list,\n",
    "    \"sweep_id\": type_list,\n",
    "    \"val_loss\": val_loss_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc12e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIDEr</th>\n",
       "      <th>lr</th>\n",
       "      <th>sweep_id</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>Full Decoder Only</td>\n",
       "      <td>10.273881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>Full Decoder Only</td>\n",
       "      <td>10.839893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.166642e-05</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>Full Decoder Only</td>\n",
       "      <td>6.952788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.328771e-08</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>Full Decoder Only</td>\n",
       "      <td>8.221014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>Full Decoder Only</td>\n",
       "      <td>9.219606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>6.292992e-01</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>rnn 512</td>\n",
       "      <td>1.442932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>6.591082e-01</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>rnn 512</td>\n",
       "      <td>1.443572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>6.985016e-01</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>rnn 512</td>\n",
       "      <td>1.494614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>8.608575e-01</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>rnn 256 bert large</td>\n",
       "      <td>0.960141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>9.045222e-01</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>rnn 256 bert large</td>\n",
       "      <td>0.930853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CIDEr        lr            sweep_id   val_loss\n",
       "0    0.000000e+00  0.000353   Full Decoder Only  10.273881\n",
       "1    0.000000e+00  0.000320   Full Decoder Only  10.839893\n",
       "2    7.166642e-05  0.000999   Full Decoder Only   6.952788\n",
       "3    1.328771e-08  0.000525   Full Decoder Only   8.221014\n",
       "4    0.000000e+00  0.000470   Full Decoder Only   9.219606\n",
       "..            ...       ...                 ...        ...\n",
       "170  6.292992e-01  0.000993             rnn 512   1.442932\n",
       "171  6.591082e-01  0.000854             rnn 512   1.443572\n",
       "172  6.985016e-01  0.000993             rnn 512   1.494614\n",
       "173  8.608575e-01  0.000527  rnn 256 bert large   0.960141\n",
       "174  9.045222e-01  0.000666  rnn 256 bert large   0.930853\n",
       "\n",
       "[175 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248abd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df.to_csv(\"runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    group.sort_values(\"CIDEr\", ascending=False).iloc[0].to_dict()\n",
    "    for _, group in runs_df.groupby(\"ft_type\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8a6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71eaaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.12.crossattention.output.dense.weight', 'bert.encoder.layer.22.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.14.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.20.crossattention.self.key.bias', 'bert.encoder.layer.20.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.15.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.14.crossattention.self.query.bias', 'bert.encoder.layer.22.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.23.crossattention.output.dense.weight', 'bert.encoder.layer.17.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.21.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.15.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.18.crossattention.output.dense.weight', 'bert.encoder.layer.22.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.19.crossattention.self.value.bias', 'bert.encoder.layer.18.crossattention.self.value.bias', 'bert.encoder.layer.15.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.20.crossattention.self.query.bias', 'bert.encoder.layer.23.crossattention.self.value.bias', 'bert.encoder.layer.20.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.15.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.23.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.20.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.21.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.17.crossattention.self.key.bias', 'bert.encoder.layer.21.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.12.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.16.crossattention.self.key.bias', 'bert.encoder.layer.22.crossattention.output.dense.bias', 'bert.encoder.layer.14.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.22.crossattention.self.query.bias', 'bert.encoder.layer.16.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.16.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.16.crossattention.self.query.weight', 'bert.encoder.layer.21.crossattention.self.value.bias', 'bert.encoder.layer.21.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.21.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.13.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.23.crossattention.self.value.weight', 'bert.encoder.layer.15.crossattention.self.query.weight', 'bert.encoder.layer.19.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.19.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.12.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.16.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.16.crossattention.self.query.bias', 'bert.encoder.layer.17.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.22.crossattention.self.query.weight', 'bert.encoder.layer.22.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.21.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.15.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.23.crossattention.self.key.bias', 'bert.encoder.layer.14.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.13.crossattention.self.value.weight', 'bert.encoder.layer.13.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.17.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.15.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.19.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.14.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.19.crossattention.self.key.weight', 'bert.encoder.layer.20.crossattention.self.key.weight', 'bert.encoder.layer.15.crossattention.output.dense.weight', 'bert.encoder.layer.22.crossattention.self.key.weight', 'bert.encoder.layer.14.crossattention.output.dense.weight', 'bert.encoder.layer.15.crossattention.self.value.bias', 'bert.encoder.layer.20.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.12.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.23.crossattention.self.query.bias', 'bert.encoder.layer.13.crossattention.self.key.bias', 'bert.encoder.layer.13.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.12.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.19.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.16.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.13.crossattention.output.dense.bias', 'bert.encoder.layer.17.crossattention.self.query.weight', 'bert.encoder.layer.20.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.12.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.16.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.17.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.22.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.12.crossattention.output.dense.bias', 'bert.encoder.layer.14.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.20.crossattention.output.dense.weight', 'bert.encoder.layer.19.crossattention.output.dense.bias', 'bert.encoder.layer.18.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.15.crossattention.self.value.weight', 'bert.encoder.layer.22.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.17.crossattention.output.dense.bias', 'bert.encoder.layer.16.crossattention.self.value.bias', 'bert.encoder.layer.13.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.21.crossattention.self.query.weight', 'bert.encoder.layer.14.crossattention.self.value.weight', 'bert.encoder.layer.18.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.18.crossattention.self.key.bias', 'bert.encoder.layer.17.crossattention.self.query.bias', 'bert.encoder.layer.16.crossattention.self.key.weight', 'bert.encoder.layer.12.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.21.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.23.crossattention.output.dense.bias', 'bert.encoder.layer.20.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.18.crossattention.self.query.weight', 'bert.encoder.layer.13.crossattention.self.value.bias', 'bert.encoder.layer.14.crossattention.output.dense.bias', 'bert.encoder.layer.17.crossattention.self.value.bias', 'bert.encoder.layer.23.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.18.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.17.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.19.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.14.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.12.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.19.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.13.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.19.crossattention.output.dense.weight', 'bert.encoder.layer.18.crossattention.output.dense.bias', 'bert.encoder.layer.21.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.23.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.18.crossattention.self.query.bias', 'bert.encoder.layer.13.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.12.crossattention.self.key.weight', 'bert.encoder.layer.18.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.23.crossattention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enables rnn-adapter tuning; num trainable params 2,234,880, ratio 0.005\n",
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /data/home/ngjhn/.local/lib/python3.8/site-packages/ ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | model     | VisionEncoderDecoderModel | 525 M \n",
      "1 | val_loss  | AvgMetric                 | 0     \n",
      "2 | test_loss | AvgMetric                 | 0     \n",
      "--------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "523 M     Non-trainable params\n",
      "525 M     Total params\n",
      "2,101.587 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5174/5174 [34:21<00:00,  2.51it/s, v_num=46, loss=1.430]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1190: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1301988.59 tokens per second.\n",
      "PTBTokenizer tokenized 58487 tokens at 463487.40 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48495, 'reflen': 48018, 'guess': [48495, 43495, 38495, 33495], 'correct': [29133, 13149, 5232, 2046]}\n",
      "ratio: 1.0099337748344162\n",
      "Bleu_1: 0.601\n",
      "Bleu_2: 0.426\n",
      "Bleu_3: 0.291\n",
      "Bleu_4: 0.197\n",
      "computing METEOR score...\n",
      "METEOR: 0.197\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.457\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.578\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [8.372 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 33.73 s\n",
      "SPICE: 0.126\n",
      "Bleu_1: 0.601\n",
      "Bleu_2: 0.426\n",
      "Bleu_3: 0.291\n",
      "Bleu_4: 0.197\n",
      "METEOR: 0.197\n",
      "ROUGE_L: 0.457\n",
      "CIDEr: 0.578\n",
      "SPICE: 0.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5174/5174 [37:32<00:00,  2.30it/s, v_num=46, loss=1.430]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5174/5174 [34:36<00:00,  2.49it/s, v_num=46, loss=1.570]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1269326.24 tokens per second.\n",
      "PTBTokenizer tokenized 58813 tokens at 467622.47 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 49023, 'reflen': 48382, 'guess': [49023, 44023, 39023, 34023], 'correct': [31765, 15630, 7097, 3085]}\n",
      "ratio: 1.0132487288660863\n",
      "Bleu_1: 0.648\n",
      "Bleu_2: 0.480\n",
      "Bleu_3: 0.347\n",
      "Bleu_4: 0.248\n",
      "computing METEOR score...\n",
      "METEOR: 0.224\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.494\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.714\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Threads( StanfordCoreNLP ) [6.319 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 29.97 s\n",
      "SPICE: 0.144\n",
      "Bleu_1: 0.648\n",
      "Bleu_2: 0.480\n",
      "Bleu_3: 0.347\n",
      "Bleu_4: 0.248\n",
      "METEOR: 0.224\n",
      "ROUGE_L: 0.494\n",
      "CIDEr: 0.714\n",
      "SPICE: 0.144\n",
      "Epoch 1: 100%|██████████| 5174/5174 [37:43<00:00,  2.29it/s, v_num=46, loss=1.570]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.084 >= min_delta = 0.0. New best score: 1.058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 5174/5174 [34:41<00:00,  2.49it/s, v_num=46, loss=1.230]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:58<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1242449.67 tokens per second.\n",
      "PTBTokenizer tokenized 57867 tokens at 450398.13 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47840, 'reflen': 47407, 'guess': [47840, 42840, 37840, 32840], 'correct': [32376, 16424, 7385, 3254]}\n",
      "ratio: 1.0091336722424746\n",
      "Bleu_1: 0.677\n",
      "Bleu_2: 0.509\n",
      "Bleu_3: 0.370\n",
      "Bleu_4: 0.266\n",
      "computing METEOR score...\n",
      "METEOR: 0.229\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.506\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.777\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [5.970 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 29.91 s\n",
      "SPICE: 0.157\n",
      "Bleu_1: 0.677\n",
      "Bleu_2: 0.509\n",
      "Bleu_3: 0.370\n",
      "Bleu_4: 0.266\n",
      "METEOR: 0.229\n",
      "ROUGE_L: 0.506\n",
      "CIDEr: 0.777\n",
      "SPICE: 0.157\n",
      "Epoch 2: 100%|██████████| 5174/5174 [37:47<00:00,  2.28it/s, v_num=46, loss=1.230]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 1.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5174/5174 [34:46<00:00,  2.48it/s, v_num=46, loss=0.975]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1316770.06 tokens per second.\n",
      "PTBTokenizer tokenized 57994 tokens at 459802.46 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48081, 'reflen': 47613, 'guess': [48081, 43081, 38081, 33081], 'correct': [32645, 16679, 7552, 3388]}\n",
      "ratio: 1.0098292483145148\n",
      "Bleu_1: 0.679\n",
      "Bleu_2: 0.513\n",
      "Bleu_3: 0.374\n",
      "Bleu_4: 0.270\n",
      "computing METEOR score...\n",
      "METEOR: 0.234\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.510\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.810\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [5.157 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 29.39 s\n",
      "SPICE: 0.161\n",
      "Bleu_1: 0.679\n",
      "Bleu_2: 0.513\n",
      "Bleu_3: 0.374\n",
      "Bleu_4: 0.270\n",
      "METEOR: 0.234\n",
      "ROUGE_L: 0.510\n",
      "CIDEr: 0.810\n",
      "SPICE: 0.161\n",
      "Epoch 3: 100%|██████████| 5174/5174 [37:51<00:00,  2.28it/s, v_num=46, loss=0.975]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.020 >= min_delta = 0.0. New best score: 1.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 5174/5174 [34:40<00:00,  2.49it/s, v_num=46, loss=1.570]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1313198.16 tokens per second.\n",
      "PTBTokenizer tokenized 59105 tokens at 462595.37 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48799, 'reflen': 47955, 'guess': [48799, 43799, 38799, 33799], 'correct': [33200, 17077, 7759, 3447]}\n",
      "ratio: 1.0175998331769156\n",
      "Bleu_1: 0.680\n",
      "Bleu_2: 0.515\n",
      "Bleu_3: 0.376\n",
      "Bleu_4: 0.271\n",
      "computing METEOR score...\n",
      "METEOR: 0.238\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.513\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.821\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Threads( StanfordCoreNLP ) [5.928 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 29.67 s\n",
      "SPICE: 0.165\n",
      "Bleu_1: 0.680\n",
      "Bleu_2: 0.515\n",
      "Bleu_3: 0.376\n",
      "Bleu_4: 0.271\n",
      "METEOR: 0.238\n",
      "ROUGE_L: 0.513\n",
      "CIDEr: 0.821\n",
      "SPICE: 0.165\n",
      "Epoch 4: 100%|██████████| 5174/5174 [37:45<00:00,  2.28it/s, v_num=46, loss=1.570]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 5174/5174 [34:37<00:00,  2.49it/s, v_num=46, loss=1.510]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1273947.31 tokens per second.\n",
      "PTBTokenizer tokenized 58845 tokens at 458121.53 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48929, 'reflen': 48146, 'guess': [48929, 43929, 38929, 33929], 'correct': [33555, 17230, 7911, 3496]}\n",
      "ratio: 1.0162630332737712\n",
      "Bleu_1: 0.686\n",
      "Bleu_2: 0.519\n",
      "Bleu_3: 0.380\n",
      "Bleu_4: 0.274\n",
      "computing METEOR score...\n",
      "METEOR: 0.239\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.516\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.839\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [5.363 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 29.48 s\n",
      "SPICE: 0.168\n",
      "Bleu_1: 0.686\n",
      "Bleu_2: 0.519\n",
      "Bleu_3: 0.380\n",
      "Bleu_4: 0.274\n",
      "METEOR: 0.239\n",
      "ROUGE_L: 0.516\n",
      "CIDEr: 0.839\n",
      "SPICE: 0.168\n",
      "Epoch 5: 100%|██████████| 5174/5174 [37:43<00:00,  2.29it/s, v_num=46, loss=1.510]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 5174/5174 [34:39<00:00,  2.49it/s, v_num=46, loss=1.060]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1247077.71 tokens per second.\n",
      "PTBTokenizer tokenized 57170 tokens at 458364.67 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 47195, 'reflen': 47034, 'guess': [47195, 42195, 37195, 32195], 'correct': [32821, 17090, 8076, 3678]}\n",
      "ratio: 1.0034230556618404\n",
      "Bleu_1: 0.695\n",
      "Bleu_2: 0.531\n",
      "Bleu_3: 0.394\n",
      "Bleu_4: 0.289\n",
      "computing METEOR score...\n",
      "METEOR: 0.243\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.521\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.871\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [4.589 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 29.18 s\n",
      "SPICE: 0.169\n",
      "Bleu_1: 0.695\n",
      "Bleu_2: 0.531\n",
      "Bleu_3: 0.394\n",
      "Bleu_4: 0.289\n",
      "METEOR: 0.243\n",
      "ROUGE_L: 0.521\n",
      "CIDEr: 0.871\n",
      "SPICE: 0.169\n",
      "Epoch 6: 100%|██████████| 5174/5174 [37:44<00:00,  2.29it/s, v_num=46, loss=1.060]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 5174/5174 [34:41<00:00,  2.49it/s, v_num=46, loss=1.610]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A/data/home/ngjhn/miniconda3/envs/awsnccl/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 307085 tokens at 1325905.07 tokens per second.\n",
      "PTBTokenizer tokenized 58838 tokens at 462994.02 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48640, 'reflen': 48052, 'guess': [48640, 43640, 38640, 33640], 'correct': [33040, 17182, 7952, 3633]}\n",
      "ratio: 1.012236743527824\n",
      "Bleu_1: 0.679\n",
      "Bleu_2: 0.517\n",
      "Bleu_3: 0.380\n",
      "Bleu_4: 0.278\n",
      "computing METEOR score...\n"
     ]
    }
   ],
   "source": [
    "from coco import *\n",
    "from adapter_utils import *\n",
    "from utils import *\n",
    "from transformers.adapters import LoRAConfig, PrefixTuningConfig\n",
    "\n",
    "\n",
    "data_module = CocoDataModule(\n",
    "    data_dir=\"/data/home/ngjhn/read/coco\",\n",
    "    ann_dir=\"/data/home/ngjhn/read/coco/annotations/\",\n",
    "    vision_model_name_or_path=\"google/vit-base-patch16-224-in21k\",\n",
    "    split_file=\"/data/home/ngjhn/read/coco/dataset_coco.json\",\n",
    "    collate_fn=collate_fn,\n",
    "    train_batch_size=16,\n",
    "    val_batch_size=128)\n",
    "\n",
    "peft_confg = RNNAdapterConf(outputs_scaling_factor=0.8551,\n",
    "                            input_dim=1024,\n",
    "                            rnn_dim=256,\n",
    "                            num_layers=1,\n",
    "                            rnn_type=\"gru\",\n",
    "                            bidirectional=False,\n",
    "                            layer_norm_before=True)\n",
    "\n",
    "model = ImageCaptioningModel(\n",
    "    vision_model_name_or_path=\"google/vit-base-patch16-224-in21k\",\n",
    "    text_model_name_or_path=\"bert-large-uncased\",\n",
    "    data_module=data_module,\n",
    "    learning_rate=0.0008540106397 * 0.70710678,\n",
    "    peft_conf=peft_confg,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\",\n",
    "                        min_delta=0.00,\n",
    "                        patience=3,\n",
    "                        verbose=True),\n",
    "        TQDMProgressBar(),\n",
    "    ],\n",
    "    default_root_dir=\"/data/home/ngjhn/read\",\n",
    "    num_sanity_val_steps=0,\n",
    "    check_val_every_n_epoch=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ad47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_from_checkpoint(\n",
    "    \"/data/home/ngjhn/read/lightning_logs/rnn_adapter/name=0-lr:config.learning_rate=0.0000-epoch=39-val_loss=1.03.ckpt\",\n",
    "    vision_model_name_or_path=\"google/vit-base-patch16-224-in21k\",\n",
    "    text_model_name_or_path=\"bert-large-uncased\",\n",
    "    data_module=data_module,\n",
    "    learning_rate=0.0008540106397,\n",
    "    peft_conf=peft_confg,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2818fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(\"cuda:0\")\n",
    "\n",
    "# RuntimeError: input.size(-1) must be equal to input_size. Expected 768, got 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a3928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awsnccl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
